import os
import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from minigrid.wrappers import ImgObsWrapper
from minigrid.core.world_object import Key, Door, Goal

# ==========================================
# 1. æ ¸å¿ƒç»„ä»¶ (CNN + Wrappers) - ä¿æŒä¸å˜
# ==========================================
class MiniGridFeaturesExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=128, normalized_image=False):
        super().__init__(observation_space, features_dim)
        n_input_channels = observation_space.shape[0]
        self.cnn = nn.Sequential(
            nn.Conv2d(n_input_channels, 16, (2, 2)), 
            nn.ReLU(),
            nn.Conv2d(16, 32, (2, 2)),
            nn.ReLU(),
            nn.Conv2d(32, 64, (2, 2)),
            nn.ReLU(),
            nn.Flatten(),
        )
        with torch.no_grad():
            sample_obs = torch.as_tensor(observation_space.sample()[None]).float()
            n_flatten = self.cnn(sample_obs).shape[1]
        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())

    def forward(self, observations):
        return self.linear(self.cnn(observations))

class SimplePotentialShaping(gym.Wrapper):
    def __init__(self, env, shaping_weight=1.0, gamma=0.99):
        super().__init__(env)
        self.shaping_weight = shaping_weight
        self.last_potential = 0.0
        self.gamma = gamma
    def get_potential(self):
        unwrapped = self.unwrapped
        agent_pos = np.array(unwrapped.agent_pos)
        goal_pos = None
        for i in range(unwrapped.grid.width):
            for j in range(unwrapped.grid.height):
                obj = unwrapped.grid.get(i, j)
                if isinstance(obj, Goal):
                    goal_pos = np.array((i, j))
                    break
            if goal_pos is not None: break
        if goal_pos is None: return 0.0
        max_dist = unwrapped.grid.width + unwrapped.grid.height
        dist = np.abs(agent_pos - goal_pos).sum()
        return 1.0 - dist / max_dist
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.last_potential = self.get_potential()
        return obs, info
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        current_potential = self.get_potential()
        shaping_reward = self.gamma * current_potential - self.last_potential
        self.last_potential = current_potential
        total_reward = reward + self.shaping_weight * shaping_reward
        return obs, total_reward, terminated, truncated, info

class HierarchicalPotentialShaping(gym.Wrapper):
    def __init__(self, env, shaping_weight=1.0, gamma=0.99):
        super().__init__(env)
        self.shaping_weight = shaping_weight
        self.last_potential = 0.0
        self.gamma = gamma
    def get_potential(self):
        unwrapped = self.unwrapped
        agent_pos = np.array(unwrapped.agent_pos)
        key_pos, door_pos, goal_pos = None, None, None
        for i in range(unwrapped.grid.width):
            for j in range(unwrapped.grid.height):
                obj = unwrapped.grid.get(i, j)
                if isinstance(obj, Key): key_pos = np.array((i, j))
                elif isinstance(obj, Door): door_pos = np.array((i, j))
                elif isinstance(obj, Goal): goal_pos = np.array((i, j))
        has_key = unwrapped.carrying is not None and isinstance(unwrapped.carrying, Key)
        door_open = False
        if door_pos is not None:
             cell = unwrapped.grid.get(*door_pos)
             if cell and cell.is_open: door_open = True
        max_dist = unwrapped.grid.width + unwrapped.grid.height
        potential = 0.0
        if door_open: 
            if goal_pos is not None:
                dist = np.abs(agent_pos - goal_pos).sum()
                potential = 2.0 + (1.0 - dist / max_dist)
        elif has_key: 
            if door_pos is not None:
                dist = np.abs(agent_pos - door_pos).sum()
                potential = 1.0 + (1.0 - dist / max_dist)
            else: potential = 1.0
        else: 
            if key_pos is not None:
                dist = np.abs(agent_pos - key_pos).sum()
                potential = 0.0 + (1.0 - dist / max_dist)
        return potential
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.last_potential = self.get_potential()
        return obs, info
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        current_potential = self.get_potential()
        shaping_reward = self.gamma * current_potential - self.last_potential
        self.last_potential = current_potential
        total_reward = reward + self.shaping_weight * shaping_reward
        return obs, total_reward, terminated, truncated, info

# ==========================================
# 2. å®éªŒé…ç½®
# ==========================================
POLICY_KWARGS = dict(
    features_extractor_class=MiniGridFeaturesExtractor,
    features_extractor_kwargs=dict(features_dim=128),
)

def make_dqn_env_fn(log_dir, algo_name, seed, wrapper_name):
    def _init():
        # [ä¿®æ”¹] é™çº§ä¸º 5x5ï¼Œç¡®ä¿å¿«é€Ÿå‡ºç»“æœ
        env = gym.make("MiniGrid-DoorKey-5x5-v0", render_mode="rgb_array")
        env = ImgObsWrapper(env)
        
        log_path = os.path.join(log_dir, f"{algo_name}_seed{seed}")
        os.makedirs(log_path, exist_ok=True)
        env = Monitor(env, filename=os.path.join(log_path, "0"))
        
        if wrapper_name == "DQN_Ours":
            env = HierarchicalPotentialShaping(env, shaping_weight=1.0)
        elif wrapper_name == "DQN_Simple":
            env = SimplePotentialShaping(env, shaping_weight=1.0)
        
        env.reset(seed=seed)
        return env
    return _init

def run_dqn_generality():
    # 5x5 æ¯”è¾ƒç®€å•ï¼Œ200k æ­¥è¶³å¤Ÿ DQN å­¦ä¼šäº†
    TOTAL_TIMESTEPS = 200_000 
    SEEDS = [201, 202] 
    # [ä¿®æ”¹] æ–‡ä»¶å¤¹åå­—æ”¹ä¸€ä¸‹ï¼Œé¿å…æ··æ·†
    LOG_DIR = "./logs_generality_dqn_5x5"
    
    print(f"ğŸ”¥ å¼€å§‹é€šç”¨æ€§éªŒè¯å®éªŒ (DQN): MiniGrid-DoorKey-5x5")
    print(f"ğŸ“‚ æ•°æ®ä¿å­˜è‡³: {LOG_DIR}")

    experiments = [
        ("DQN_Baseline", "None"),
        ("DQN_Simple", "Simple"), # åœ¨ 5x5 ä¸Š Simple å¯èƒ½ä¹Ÿä¼šå­¦å‡ºæ¥ï¼Œä½† Ours åº”è¯¥æ›´å¿«
        ("DQN_Ours", "Ours")
    ]

    for algo_name, wrapper_name in experiments:
        for seed in SEEDS:
            print(f"\nğŸš€ Training [{algo_name}] Seed {seed}...")
            
            env = DummyVecEnv([make_dqn_env_fn(LOG_DIR, algo_name, seed, wrapper_name)])
            
            # DQN é’ˆå¯¹ MiniGrid çš„æ¨èå‚æ•°
            model = DQN(
                "CnnPolicy",
                env,
                verbose=1,
                seed=seed,
                policy_kwargs=POLICY_KWARGS,
                learning_rate=1e-4,
                buffer_size=100000,
                learning_starts=1000, # å…ˆéšæœºæ¢ç´¢ä¸€ä¼š
                batch_size=32,
                tau=1.0,
                target_update_interval=1000,
                train_freq=4,
                gradient_steps=1,
                exploration_fraction=0.1, # å‰10%æ—¶é—´è¿›è¡Œæ¢ç´¢è¡°å‡
                exploration_final_eps=0.05,
                gamma=0.99
            )
            
            model.learn(total_timesteps=TOTAL_TIMESTEPS)
            env.close()
    
    print(f"\nâœ… DQN å®éªŒç»“æŸï¼è¯·ä¿®æ”¹ç»˜å›¾è„šæœ¬çš„ LOG_DIR æŸ¥çœ‹ç»“æœã€‚")

if __name__ == "__main__":
    # 1. å…ˆéªŒè¯
    # verify_dqn_setup() # ç¬¬ä¸€æ¬¡è¿è¡Œå»ºè®®æ‰“å¼€æ³¨é‡Š
    
    # 2. åè®­ç»ƒ
    run_dqn_generality()